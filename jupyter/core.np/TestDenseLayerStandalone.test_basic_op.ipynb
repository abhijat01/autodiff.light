{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correponding pyunit test case is in \n",
    "<b>tests.core.np.TestDenseLayer.DenseLayerStandAlone#test_basic_op</b>\n",
    "\n",
    "Note that given $w, b$ and $x$, we calculate \n",
    "\n",
    "$y=wx+b$ \n",
    "\n",
    "whereas you need to provide  $x^T, w $ and $b$  to pytorch to caculate \n",
    "\n",
    "$y_{torch}=xw^T+b$  in which case, $y=y^{T}_{torch}$\n",
    "\n",
    "Strictly speaking, shape of $b$ should matter but I have not yet tested it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "import torch.optim as optim \n",
    "import sys \n",
    "import os \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.dense =  nn.Linear(3,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net() \n",
    "print(\"Printing dense layer weights...\")\n",
    "print(net.dense.weight.data)\n",
    "print(net.dense.bias.data)\n",
    "print(\"finished printing dense layer weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Manually set the layer weights</h2>\n",
    "Setting the weights to match those used in the pyunit test case. This is usually better than \n",
    "relying on seeding random number generators. \n",
    "\n",
    "Also computing the output directly as \n",
    "\n",
    "$y_{pred}=xw^{T}+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w = np.array([[1, 3, -1], [0, -4, 2]])\n",
    "model_b = np.array([-3, 2])\n",
    "w_tensor = torch.from_numpy(model_w).float()\n",
    "b_tensor = torch.from_numpy(model_b).float() \n",
    "w = nn.Parameter(w_tensor)\n",
    "b = nn.Parameter(b_tensor)\n",
    "net.dense.weight  = w \n",
    "net.dense.bias = b\n",
    "x = np.array([[1, -1], [2, 3], [-1, -2]]).T\n",
    "print(\"#------ \")\n",
    "print(\"model_w=np.{}\".format(repr(model_w)))\n",
    "print(\"model_b=np.{}\".format(repr(model_b)))\n",
    "print(\"x=np.{}\".format(repr(x)))\n",
    "yy = x@model_w.T + model_b \n",
    "print(\"# ---- expected final value (directly computed)----\")\n",
    "print(\"y-predicted=np.{}\".format(repr(yy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.from_numpy(x).float() \n",
    "print(\"Input Shape:{}\".format(input.shape))\n",
    "output = net(input)\n",
    "print(\"Output:{}\".format(output.data))\n",
    "y = np.array([[-1, 1], [-3, -1]]).T\n",
    "target = torch.from_numpy(y).float()\n",
    "print(\"Target:{}\".format(target.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = nn.MSELoss()\n",
    "loss = criteria(output, target)\n",
    "print(\"loss: {}\".format(loss))\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "# perform a backward pass (backpropagation)\n",
    "loss.backward()\n",
    "# Update the parameters\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=8, sci_mode=False)\n",
    "print(\"Printing wight and weight gradient after one step\")\n",
    "print(\"Weight:{}\".format(net.dense.weight.data))\n",
    "print(\"W Grad:{}\".format(net.dense.weight.grad.data))\n",
    "print(\"\\nPrinting bias and bias gradient after one step\")\n",
    "print(\"Bias:{}\".format(net.dense.bias.data))\n",
    "print(\"B grad:{}\".format(net.dense.bias.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing test_basic_op_large_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x:(4, 6), shape of w:(3, 4), shape of y_pred:(3, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "x=np.array([[0.54566752, 0.66921034, 0.35265542, 0.32324271, 0.35036963,\n",
    "        0.05317591],\n",
    "       [0.97433629, 0.5027976 , 0.15637831, 0.72948084, 0.42097552,\n",
    "        0.52522781],\n",
    "       [0.41793729, 0.48112345, 0.46862087, 0.88918467, 0.48792933,\n",
    "        0.32439625],\n",
    "       [0.4775774 , 0.58105899, 0.35079832, 0.79657794, 0.3910011 ,\n",
    "        0.72908915]])\n",
    "w=np.array([[0.61013274, 0.86914947, 0.95211922, 0.96385655],\n",
    "       [0.64290252, 0.2717017 , 0.193146  , 0.05004571],\n",
    "       [0.14360354, 0.54256991, 0.90870491, 0.06577582]])\n",
    "b=np.array([[0.76026806],\n",
    "       [0.32982798],\n",
    "       [0.01258297]])\n",
    "y_pred = w@x+b\n",
    "y_target = np.ones_like(y_pred)\n",
    "print(\"shape of x:{}, shape of w:{}, shape of y_pred:{}\".format(x.shape, w.shape, y_pred.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.dense =  nn.Linear(4,3)\n",
    "    def set_weights(self, w_numpy, b_numpy):\n",
    "        w_tensor= torch.from_numpy(w_numpy).float() \n",
    "        w_param = nn.Parameter(w_tensor)\n",
    "        b_tensor = torch.from_numpy(b_numpy).float() \n",
    "        b_param = nn.Parameter(b_tensor)\n",
    "        self.dense.weight = w_param \n",
    "        self.dense.bias  = b_param \n",
    "    def forward(self, x):\n",
    "        return self.dense(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred=tensor([[2.79828358, 1.04999149, 1.03078306],\n",
      "        [2.62372589, 1.01868248, 0.85690564],\n",
      "        [1.89565301, 0.70710748, 0.59698385],\n",
      "        [3.20591354, 0.94745052, 1.31519830],\n",
      "        [2.18136644, 0.78327084, 0.76040810],\n",
      "        [2.26081514, 0.60586381, 0.64792889]])\n",
      "y_target:tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "net2 = Net2() \n",
    "net2.set_weights(w, b.reshape(-1))\n",
    "input = torch.from_numpy(x.T).float()\n",
    "input.requires_grad = True \n",
    "output = net2(input)\n",
    "print(\"y_pred={}\".format(output.data))\n",
    "target = torch.ones_like(output, requires_grad=True)\n",
    "print(\"y_target:{}\".format(target.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8490058183670044\n"
     ]
    }
   ],
   "source": [
    "criteria = nn.MSELoss()\n",
    "loss = criteria(output, target)\n",
    "print(\"loss: {}\".format(loss))\n",
    "optimizer = torch.optim.SGD(net2.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "Printing original weight and bias\n",
      "---------------------------------------------------\n",
      "Weight:tensor([[0.61013275, 0.86914945, 0.95211923, 0.96385658],\n",
      "        [0.64290249, 0.27170169, 0.19314601, 0.05004571],\n",
      "        [0.14360353, 0.54256994, 0.90870494, 0.06577582]])\n",
      "Bias:tensor([0.76026803, 0.32982799, 0.01258297])\n",
      "---------------------------------------------------\n",
      "Printing wight and weight gradient after one step\n",
      "---------------------------------------------------\n",
      "Weight:tensor([[0.60973525, 0.86854088, 0.95157486, 0.96327269],\n",
      "        [0.64292222, 0.27173772, 0.19318908, 0.05009926],\n",
      "        [0.14362818, 0.54258782, 0.90872669, 0.06581017]])\n",
      "W Grad:tensor([[ 0.39752683,  0.60859025,  0.54437733,  0.58387089],\n",
      "        [-0.01970989, -0.03603142, -0.04307830, -0.05355303],\n",
      "        [-0.02465229, -0.01786957, -0.02174304, -0.03434603]])\n",
      "\n",
      "Printing bias and bias gradient after one step\n",
      "Bias:tensor([0.75927186, 0.32992661, 0.01267095])\n",
      "B grad:tensor([ 0.99619532, -0.09862594, -0.08797690])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=8, sci_mode=False)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Printing original weight and bias\")\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Weight:{}\".format(net2.dense.weight.data))\n",
    "print(\"Bias:{}\".format(net2.dense.bias.data))\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Printing wight and weight gradient after one step\")\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Weight:{}\".format(net2.dense.weight.data))\n",
    "print(\"W Grad:{}\".format(net2.dense.weight.grad.data))\n",
    "print(\"\\nPrinting bias and bias gradient after one step\")\n",
    "print(\"Bias:{}\".format(net2.dense.bias.data))\n",
    "print(\"B grad:{}\".format(net2.dense.bias.grad.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.60973525, 0.86854088, 0.95157486, 0.96327269],\n",
      "        [0.64292222, 0.27173772, 0.19318908, 0.05009926],\n",
      "        [0.14362818, 0.54258782, 0.90872669, 0.06581017]])\n"
     ]
    }
   ],
   "source": [
    "w_tensor=torch.from_numpy(w).float() \n",
    "w_new =w_tensor - 0.001*net2.dense.weight.grad\n",
    "print(w_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2=w_new.numpy() \n",
    "w1=w\n",
    "np.sum(np.square(w1-w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "list(net2.parameters())\n",
    "print(input.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
