{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "import torch.optim as optim \n",
    "import sys \n",
    "import os \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.dense =  nn.Linear(3,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing dense layer weights...\n",
      "tensor([[-0.0117,  0.1319,  0.4879],\n",
      "        [ 0.4381, -0.2224,  0.4127]])\n",
      "tensor([-0.1199,  0.4131])\n",
      "finished printing dense layer weights\n"
     ]
    }
   ],
   "source": [
    "net = Net() \n",
    "print(\"Printing dense layer weights...\")\n",
    "print(net.dense.weight.data)\n",
    "print(net.dense.bias.data)\n",
    "print(\"finished printing dense layer weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Manually set the layer weights</h2>\n",
    "Setting the weights to match those used in the pyunit test case. This is usually better than \n",
    "relying on random seeds. \n",
    "\n",
    "Also computing the output directly as \n",
    "\n",
    "$y_{pred}=xw^{T}+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------ \n",
      "model_w=np.array([[ 1,  3, -1],\n",
      "       [ 0, -4,  2]])\n",
      "model_b=np.array([-3,  2])\n",
      "x=np.array([[ 1,  2, -1],\n",
      "       [-1,  3, -2]])\n",
      "# ---- expected final value (directly computed)----\n",
      "y-predicted=np.array([[  5,  -8],\n",
      "       [  7, -14]])\n"
     ]
    }
   ],
   "source": [
    "model_w = np.array([[1, 3, -1], [0, -4, 2]])\n",
    "model_b = np.array([-3, 2])\n",
    "w_tensor = torch.from_numpy(model_w).float()\n",
    "b_tensor = torch.from_numpy(model_b).float() \n",
    "w = nn.Parameter(w_tensor)\n",
    "b = nn.Parameter(b_tensor)\n",
    "net.dense.weight  = w \n",
    "net.dense.bias = b\n",
    "x = np.array([[1, -1], [2, 3], [-1, -2]]).T\n",
    "print(\"#------ \")\n",
    "print(\"model_w=np.{}\".format(repr(model_w)))\n",
    "print(\"model_b=np.{}\".format(repr(model_b)))\n",
    "print(\"x=np.{}\".format(repr(x)))\n",
    "yy = x@model_w.T + model_b \n",
    "print(\"# ---- expected final value (directly computed)----\")\n",
    "print(\"y-predicted=np.{}\".format(repr(yy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:torch.Size([2, 3])\n",
      "Output:tensor([[  5.,  -8.],\n",
      "        [  7., -14.]])\n",
      "Target:tensor([[-1.,  1.],\n",
      "        [-3., -1.]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.from_numpy(x).float() \n",
    "print(\"Input Shape:{}\".format(input.shape))\n",
    "output = net(input)\n",
    "print(\"Output:{}\".format(output.data))\n",
    "y = np.array([[-1, 1], [-3, -1]])\n",
    "target = torch.from_numpy(y).float()\n",
    "print(\"Target:{}\".format(target.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 96.5\n"
     ]
    }
   ],
   "source": [
    "criteria = nn.MSELoss()\n",
    "loss = criteria(output, target)\n",
    "print(\"loss: {}\".format(loss))\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "# perform a backward pass (backpropagation)\n",
    "loss.backward()\n",
    "# Update the parameters\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the wight and weight gradient after one step\n",
      "Weight:tensor([[ 1.0020e+00,  2.9790e+00, -9.8700e-01],\n",
      "        [-2.0000e-03, -3.9715e+00,  1.9825e+00]])\n",
      "W Grad:tensor([[ -2.0000,  21.0000, -13.0000],\n",
      "        [  2.0000, -28.5000,  17.5000]])\n",
      "\n",
      "Printing bias and bias gradient after one step\n",
      "Bias:tensor([-3.0080,  2.0110])\n",
      "B grad:tensor([  8., -11.])\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing wight and weight gradient after one step\")\n",
    "print(\"Weight:{}\".format(net.dense.weight.data))\n",
    "print(\"W Grad:{}\".format(net.dense.weight.grad.data))\n",
    "print(\"\\nPrinting bias and bias gradient after one step\")\n",
    "print(\"Bias:{}\".format(net.dense.bias.data))\n",
    "print(\"B grad:{}\".format(net.dense.bias.grad.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
