{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correponding pyunit test case is in \n",
    "<b>tests.core.np.TestDenseLayer.DenseLayerStandAlone#test_basic_op</b>\n",
    "\n",
    "Note that given $w, b$ and $x$, we calculate \n",
    "\n",
    "$y=wx+b$ \n",
    "\n",
    "whereas you need to provide  $x^T, w $ and $b$  to pytorch to caculate \n",
    "\n",
    "$y_{torch}=xw^T+b$  in which case, $y=y^{T}_{torch}$\n",
    "\n",
    "Strictly speaking, shape of $b$ should matter but I have not yet tested it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "import torch.optim as optim \n",
    "import sys \n",
    "import os \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.dense =  nn.Linear(3,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing dense layer weights...\n",
      "tensor([[ 0.3781,  0.0311, -0.0762],\n",
      "        [-0.3608,  0.4820,  0.3797]])\n",
      "tensor([ 0.1245, -0.1654])\n",
      "finished printing dense layer weights\n"
     ]
    }
   ],
   "source": [
    "net = Net() \n",
    "print(\"Printing dense layer weights...\")\n",
    "print(net.dense.weight.data)\n",
    "print(net.dense.bias.data)\n",
    "print(\"finished printing dense layer weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Manually set the layer weights</h2>\n",
    "Setting the weights to match those used in the pyunit test case. This is usually better than \n",
    "relying on seeding random number generators. \n",
    "\n",
    "Also computing the output directly as \n",
    "\n",
    "$y_{pred}=xw^{T}+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------ \n",
      "model_w=np.array([[ 1,  3, -1],\n",
      "       [ 0, -4,  2]])\n",
      "model_b=np.array([-3,  2])\n",
      "x=np.array([[ 1,  2, -1],\n",
      "       [-1,  3, -2]])\n",
      "# ---- expected final value (directly computed)----\n",
      "y-predicted=np.array([[  5,  -8],\n",
      "       [  7, -14]])\n"
     ]
    }
   ],
   "source": [
    "model_w = np.array([[1, 3, -1], [0, -4, 2]])\n",
    "model_b = np.array([-3, 2])\n",
    "w_tensor = torch.from_numpy(model_w).float()\n",
    "b_tensor = torch.from_numpy(model_b).float() \n",
    "w = nn.Parameter(w_tensor)\n",
    "b = nn.Parameter(b_tensor)\n",
    "net.dense.weight  = w \n",
    "net.dense.bias = b\n",
    "x = np.array([[1, -1], [2, 3], [-1, -2]]).T\n",
    "print(\"#------ \")\n",
    "print(\"model_w=np.{}\".format(repr(model_w)))\n",
    "print(\"model_b=np.{}\".format(repr(model_b)))\n",
    "print(\"x=np.{}\".format(repr(x)))\n",
    "yy = x@model_w.T + model_b \n",
    "print(\"# ---- expected final value (directly computed)----\")\n",
    "print(\"y-predicted=np.{}\".format(repr(yy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:torch.Size([2, 3])\n",
      "Output:tensor([[  5.,  -8.],\n",
      "        [  7., -14.]])\n",
      "Target:tensor([[-1., -3.],\n",
      "        [ 1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.from_numpy(x).float() \n",
    "print(\"Input Shape:{}\".format(input.shape))\n",
    "output = net(input)\n",
    "print(\"Output:{}\".format(output.data))\n",
    "y = np.array([[-1, 1], [-3, -1]]).T\n",
    "target = torch.from_numpy(y).float()\n",
    "print(\"Target:{}\".format(target.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 66.5\n"
     ]
    }
   ],
   "source": [
    "criteria = nn.MSELoss()\n",
    "loss = criteria(output, target)\n",
    "print(\"loss: {}\".format(loss))\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "# perform a backward pass (backpropagation)\n",
    "loss.backward()\n",
    "# Update the parameters\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing wight and weight gradient after one step\n",
      "Weight:tensor([[ 1.00000000,  2.98499990, -0.99100000],\n",
      "        [-0.00400000, -3.97550011,  1.98450005]])\n",
      "W Grad:tensor([[  0.00000000,  15.00000000,  -9.00000000],\n",
      "        [  4.00000000, -24.50000000,  15.50000000]])\n",
      "\n",
      "Printing bias and bias gradient after one step\n",
      "Bias:tensor([-3.00600004,  2.00900006])\n",
      "B grad:tensor([ 6., -9.])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=8, sci_mode=False)\n",
    "print(\"Printing wight and weight gradient after one step\")\n",
    "print(\"Weight:{}\".format(net.dense.weight.data))\n",
    "print(\"W Grad:{}\".format(net.dense.weight.grad.data))\n",
    "print(\"\\nPrinting bias and bias gradient after one step\")\n",
    "print(\"Bias:{}\".format(net.dense.bias.data))\n",
    "print(\"B grad:{}\".format(net.dense.bias.grad.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
